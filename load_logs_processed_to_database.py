import os
from io import StringIO
from typing import Optional
from pathlib import Path
import psycopg2
from dotenv import load_dotenv

import polars as pl

# Load environment variables from .env so local runs work without exporting.
load_dotenv()

# Project root directory (3 levels up from this script)
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent

def get_db_connection(
    host: Optional[str] = None,
    port: Optional[str] = None,
    dbname: Optional[str] = None,
    user: Optional[str] = None,
    password: Optional[str] = None,
):
    """
    Open a Postgres connection using psycopg2.

    - Parameters override environment variables; if omitted, we read env vars.
    - Env vars: POSTGRES_HOST (default "localhost"), POSTGRES_PORT (default "5432"),
      POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD.
    - Autocommit stays OFF so callers control commit/rollback explicitly.
    """
    conn = psycopg2.connect(
        host=host or os.getenv("POSTGRES_HOST", "localhost"),
        port=port or os.getenv("POSTGRES_PORT", "5432"),
        database=dbname or os.getenv("POSTGRES_DB", "across_analytics"),
        user=user or os.getenv("POSTGRES_USER"),
        password=password or os.getenv("POSTGRES_PASSWORD"),
    )

    conn.autocommit = False
    return conn


def create_raw_table(conn, table_name: str) -> None:
    """
    Create the raw (bronze) landing table.
    """

    # first create schema if not exists
    # then create table if not exists
    create_sql = f"""
    CREATE SCHEMA IF NOT EXISTS raw;
    CREATE TABLE IF NOT EXISTS raw.{table_name} (
    ...Look at the project context architecture and tell me what's better:
1. Load raw files to Postgres
2. First transform them (converting hex into human-readable format) and then load them into Postgres
Explain only, don't create any files.
    """
    with conn.cursor() as cur:
        cur.execute(create_sql)
    conn.commit()

    return table_name


def truncate_raw_table(conn, table_name: str) -> None:
    """
    Remove all rows from a raw table (keeps table structure).
    Use for full refresh strategy.
    """
    with conn.cursor() as cur:
        cur.execute(f"TRUNCATE TABLE raw.{table_name};")
    conn.commit()
    print(f"Truncated raw.{table_name}")


def load_parquet_to_raw_copy(conn, parquet_path: str, table_name: str) -> int:
    """
    COPY-based file loader: stream Parquet -> CSV -> Postgres COPY STDIN
    """
    # read parquet file into polars dataframe
    df = pl.read_parquet(parquet_path)
    if df.is_empty():
        print("No rows to load; Parquet is empty.")
        return 0

    columns = [

    ]

    # Add blockchain, api_extracted_start_date, api_extracted_end_date columns with the values from the parquet file
    df = df.with_columns(pl.lit(os.path.basename(parquet_path).split("_")[1]).alias("blockchain"))
    df = df.with_columns(pl.lit(os.path.basename(parquet_path).split("_")[2].split(".")[0]).alias("api_extracted_start_date"))
    df = df.with_columns(pl.lit(os.path.basename(parquet_path).split("_")[4].split(".")[0]).alias("api_extracted_end_date"))

    # Add source_file column with the name of corresponding parquet file
    df = df.with_columns(pl.lit(os.path.basename(parquet_path)).alias("source_file"))

    # Ensure all columns exist; missing ones become NULL
    for col in columns:
        if col not in df.columns:
            df = df.with_columns(pl.lit(None).alias(col))
    df = df.select(columns)

    # Stream DataFrame to CSV in-memory for COPY
    buffer = StringIO()
    df.write_csv(buffer, include_header=False)
    buffer.seek(0)

    copy_sql = f"""
        COPY raw.{table_name} ({", ".join(columns)})
        FROM STDIN WITH (FORMAT CSV)
    """

    with conn.cursor() as cur:
        cur.copy_expert(sql=copy_sql, file=buffer)
    conn.commit()

    inserted = len(df)
    print(f"COPY inserted {inserted} rows from {parquet_path} into raw.{table_name}")
    return inserted


def check_table_loaded(conn, table_name: str, source_file: Optional[str] = None) -> dict:
    """
    Check if table was loaded and return status information.
    
    Args:
        conn: psycopg2 connection
        table_name: Name of the table to check (default: across_bridge_logs_raw)
        source_file: Optional filename to check if it was already loaded
    
    Returns:
        - exists: bool (does table exist?)
        - row_count: int (total rows, 0 if table doesn't exist)
        - file_loaded: bool (was source_file loaded? None if source_file not provided)
    """
    result = {
        "exists": False,
        "row_count": 0,
        "file_loaded": None
    }
    
    with conn.cursor() as cur:
        # Check if table exists
        cur.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'raw' 
                AND table_name = %s
            );
        """, (table_name,))
        table_exists = cur.fetchone()[0]
        result["exists"] = table_exists
        
        if not table_exists:
            return result
        
        # Count total rows
        cur.execute(f"SELECT COUNT(*) FROM raw.{table_name};")
        result["row_count"] = cur.fetchone()[0]
        
        # Check if specific file was loaded (if provided)
        if source_file:
            cur.execute(f"""
                SELECT COUNT(*) > 0 
                FROM raw.{table_name} 
                WHERE source_file = %s;
            """, (source_file,))
            result["file_loaded"] = cur.fetchone()[0]
    
    return result



def load_all_parquet_files_to_raw_tables(
    processed_dir: Optional[Path] = None,
) -> int:
    """
    Airflow-friendly wrapper to load all Parquet files in a directory into raw.

    - Makes its own DB connection, commits/rolls back, and closes cleanly.
    - Skips files already ingested (checks source_file).
    - Returns total rows inserted across all files.
    """
    if processed_dir is None:
        processed_dir = PROJECT_ROOT / "data/processed"

    # Collect parquet files once; deterministic ordering helps reproducibility.
    parquet_files = sorted(Path(processed_dir).glob("*.parquet"))
    if not parquet_files:
        print(f"No parquet files found in {processed_dir}")
        return 0

    # Sort files by chain to optimize DB operations (truncate once per chain)
    # Filename format: logs_<chain>_....parquet
    files_by_chain = {}
    for p_file in parquet_files:
        try:
            chain = p_file.name.split("_")[1]
            if chain not in files_by_chain:
                files_by_chain[chain] = []
            files_by_chain[chain].append(p_file)
        except IndexError:
            print(f"Skipping malformed filename: {p_file.name}")
            continue

    # Open a single DB connection for the entire batch; close at the end.
    conn = get_db_connection()

    total_inserted = 0
    try:
        # Process one chain at a time
        for chain, chain_files in files_by_chain.items():
            print(f"\nProcessing chain: {chain} ({len(chain_files)} files)")
            
            table_name = f"{chain}_logs_processed"
            
            # Step 1: Create table if missing
            try:
                create_raw_table(conn, table_name)
                print(f"✓ Table raw.{table_name} ensured.")
            except Exception as e:
                conn.rollback()
                print(f"❌ Error creating table {table_name}: {e}")
                continue

            # Step 2: Truncate ONCE per chain (Full Refresh Strategy)
            try:
                truncate_raw_table(conn, table_name)
            except Exception as e:
                conn.rollback()
                print(f"❌ Error truncating {table_name}: {e}")
                continue

            # Step 3: Load ALL files for this chain (Append)
            for p_file in chain_files:
                try:
                    inserted = load_parquet_to_raw_copy(conn, p_file, table_name)
                    print(f"  ✓ Loaded {inserted:,} rows from {p_file.name}")
                    total_inserted += inserted
                except Exception as e:
                    conn.rollback()
                    print(f"  ❌ Error loading {p_file.name}: {e}")
                    # Decide: fail the whole chain? or skip file?
                    # For now, we skip the file but warn deeply.
            
            # Commit after processing all files for the chain
            conn.commit()
            print(f"✓ Committed all data for {chain}.\n")

        print(f"Total rows loaded across all tables: {total_inserted:,}")
        return total_inserted
    finally:
        conn.close()


if __name__ == "__main__":
    load_all_parquet_files_to_raw_tables()